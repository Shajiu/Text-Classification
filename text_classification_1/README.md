# AuxiliaryTools
本脚本中包含一些NLP任务中基本的操作:分句(英文)、分词(中、英)、识别语种(所有).
## 一、Participle_word_tool
#### 1.[Participle_word_tool.py:](https://github.com/Shajiu/AuxiliaryTools/blob/master/Participle_word_tool/divide_paper.py) 英文分词工具
#### 2.[divide_sentence.py:](https://github.com/Shajiu/AuxiliaryTools/blob/master/Participle_word_tool/divide_sentence.py)英文分句工具
#### 3.[jieba_fenCi.py:](https://github.com/Shajiu/AuxiliaryTools/blob/master/Participle_word_tool/jieba_fenCi.py)中文分词工具
#### 4.[participle.py](https://github.com/Shajiu/AuxiliaryTools/blob/master/Participle_word_tool/participle.py)英文分词工具
#### 5.[sentence_token.py:](https://github.com/Shajiu/AuxiliaryTools/blob/master/Participle_word_tool/sentence_token.py)英文分词、英文分句、中文分词、识别语种
 
